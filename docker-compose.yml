services:
  web:
    build: .
    command: bundle exec rails s -p 3000 -b '0.0.0.0'
    volumes:
      - .:/app
      - node_modules:/app/node_modules
    ports:
      # This port only needs to be exposed for local development.
      # In production, the web container is behind a reverse proxy (Caddy).
      - "3000:3000"
    depends_on:
      - js
    env_file:
      - .env
  js:
    build: .
    command: npm run build:watch
    volumes:
      - .:/app
      - node_modules:/app/node_modules
    env_file:
      - .env
  sidekiq:
    build: .
    command: bundle exec sidekiq -C config/sidekiq.yml
    volumes:
      - .:/app
    depends_on:
      - redis
    env_file:
      - .env
  redis:
    image: redis:6.2-alpine
    volumes:
      - redis_data:/data
  # Reverse proxy options - set HOST_MODE in .env to "caddy", "ngrok", or "direct"
  # Only one runs at a time based on the profile selected by start.sh
  caddy:
    image: caddy:2-alpine
    profiles: ["caddy"]
    command: ["caddy", "run", "--config", "/etc/caddy/Caddyfile", "--adapter", "caddyfile"]
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - web
    env_file:
      - .env
    environment:
      HOSTNAME: "${HOSTNAME}"
    volumes:
      - caddy_data:/data
      - caddy_config:/config
      - ./Caddyfile:/etc/caddy/Caddyfile
  ngrok:
    image: ngrok/ngrok:latest
    profiles: ["ngrok"]
    restart: unless-stopped
    depends_on:
      - web
    command: http web:3000
    env_file:
      # Make sure NGROK_AUTHTOKEN is set in .env
      - .env
  # LLM services for in-app AI chat (optional - use profiles to enable)
  ollama:
    image: ollama/ollama:latest
    profiles: ["llm"]
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        limits:
          memory: 4G
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    profiles: ["llm"]
    command: ["--config", "/app/litellm_config.yaml"]
    ports:
      - "4000:4000"
    volumes:
      - ./config/litellm_config.yaml:/app/litellm_config.yaml
    depends_on:
      - ollama
    env_file:
      - .env
    deploy:
      resources:
        limits:
          memory: 512M

volumes:
  redis_data:
  caddy_data:
  caddy_config:
  node_modules:
  ollama_data:
