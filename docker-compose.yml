# Development Docker Compose configuration
#
# This is for local development only. For production, use docker-compose.production.yml.

services:
  web:
    build: .
    command: bundle exec rails s -p 3000 -b '0.0.0.0'
    user: "1000:1000"
    volumes:
      - .:/app
      - node_modules:/app/node_modules
      - gem_cache:/gems
    ports:
      - "3000:3000"
    depends_on:
      db:
        condition: service_started
      js:
        condition: service_started
      redis:
        condition: service_healthy
    env_file:
      - .env
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/healthcheck"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - frontend
      - backend

  js:
    build: .
    command: npm run build:watch
    user: "1000:1000"
    volumes:
      - .:/app
      - node_modules:/app/node_modules
    env_file:
      - .env
    networks:
      - backend

  sidekiq:
    build: .
    command: bundle exec sidekiq -C config/sidekiq.yml
    user: "1000:1000"
    volumes:
      - .:/app
      - gem_cache:/gems
    depends_on:
      db:
        condition: service_started
      redis:
        condition: service_healthy
    env_file:
      - .env
    networks:
      - backend

  # Local PostgreSQL database (production uses managed database)
  db:
    image: postgres:13
    volumes:
      - pg_data:/var/lib/postgresql/data
    env_file:
      - .env
    networks:
      - backend

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - backend

  # Email catcher for development (view emails at http://localhost:1080)
  mailcatcher:
    image: sj26/mailcatcher
    ports:
      - "1080:1080"
    environment:
      MAILCATCHER_SMTP_PORT: 1025
      MAILCATCHER_HTTP_PORT: 1080
    networks:
      - backend

  # Reverse proxy options - set HOST_MODE in .env to "caddy", "ngrok", or "direct"
  caddy:
    image: caddy:2-alpine
    profiles: ["caddy"]
    command: ["caddy", "run", "--config", "/etc/caddy/Caddyfile", "--adapter", "caddyfile"]
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      web:
        condition: service_healthy
    env_file:
      - .env
    environment:
      HOSTNAME: "${HOSTNAME}"
    volumes:
      - caddy_data:/data
      - caddy_config:/config
      - ./Caddyfile:/etc/caddy/Caddyfile
    networks:
      - frontend

  ngrok:
    image: ngrok/ngrok:latest
    profiles: ["ngrok"]
    restart: unless-stopped
    depends_on:
      - web
    command: http web:3000
    env_file:
      - .env
    networks:
      - frontend

  # Optional: ClamAV virus scanner for file uploads (resource-intensive)
  # clamav:
  #   image: clamav/clamav:stable
  #   platform: linux/amd64
  #   volumes:
  #     - clamav_data:/var/lib/clamav
  #   networks:
  #     - backend

  # LLM services for in-app AI chat (optional - use profiles to enable)
  ollama:
    image: ollama/ollama:latest
    profiles: ["llm"]
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        limits:
          memory: 4G
    networks:
      - backend

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    profiles: ["llm"]
    command: ["--config", "/app/litellm_config.yaml"]
    ports:
      - "4000:4000"
    volumes:
      - ./config/litellm_config.yaml:/app/litellm_config.yaml
    depends_on:
      - ollama
    env_file:
      - .env
    deploy:
      resources:
        limits:
          memory: 512M
    networks:
      - backend

  trio:
    image: ghcr.io/ibis-coordination/trio:latest
    profiles: ["llm"]
    ports:
      - "8000:8000"
    depends_on:
      - litellm
    env_file:
      - .env
    deploy:
      resources:
        limits:
          memory: 256M
    networks:
      - backend

networks:
  frontend:
  backend:
    internal: true

volumes:
  pg_data:
  redis_data:
  caddy_data:
  caddy_config:
  node_modules:
  gem_cache:
  ollama_data:
  # clamav_data:
